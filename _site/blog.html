<!doctype html>
<html>
  <link rel="stylesheet" href="assets/css/jemdoc.css" type="text/css" />
  <head>
    <meta charset="utf-8">
    <title>Kaizhao Liu (刘锴昭)</title>
  </head>
  <body>
    <table summary="Table for page layout." id="tlayout">
      <tr valign="top">
        <td id="layout-menu">
          <div class="menu-category">Kaizhao Liu</div>
          <nav>
  
    <div class="menu-item">
      <a href="/" >
      Home
      </a>
    </div>
  
    <div class="menu-item">
      <a href="/publications.html" >
      Publications
      </a>
    </div>
  
    <div class="menu-item">
      <a href="/blog.html" style="color: black;">
      Blog
      </a>
    </div>
  
</nav>
        </td>
        <td id="layout-content">
          <h3></h3>
<h1>Latest Posts</h1>

<ul>
  
    <li>
      <h2><a href="/2025/03/25/numerical-analysis.html">Numerical Analysis</a></h2>
      <p>In this note, I sketch the most fundamental ideas and algorithms developed in the traditional field of numerical analysis. If permitted, I also sketch the mathematical analysis of such algorithms. The material means to establish an understanding of this field as easy as possible, thus all implementation and technical details are omitted.</p>

    </li>
  
    <li>
      <h2><a href="/2025/03/25/numerical-algebra.html">Numerical Algebra</a></h2>
      <p>In this note, I sketch the most fundamental ideas and algorithms developed in the traditional field of numerical algebra. If permitted, I also sketch the mathematical analysis of such algorithms. The material means to establish an understanding of this field as easy as possible, thus all implementation and technical details are omitted.</p>

    </li>
  
    <li>
      <h2><a href="/2025/03/21/nystrom-approximation.html">Nystrom</a></h2>
      <p>Algebra view: matrix, column space
Geometric view: operators, image</p>

    </li>
  
    <li>
      <h2><a href="/2025/03/02/anytime-acceleration-gd.html">Anytime Acceleration of Gradient Descent</a></h2>
      <p>This is my note on the paper <a href="arxiv.org/abs/2411.17668">Anytime Acceleration of Gradient Descent</a>.</p>

    </li>
  
    <li>
      <h2><a href="/2025/02/23/what-I-read.html">What I've been reading</a></h2>
      <p>The following is a list of various things that I have read (to some extent) since starting graduate school. 
I haven’t necessarily read all (or even most) of any particular item on the list; this is just meant to record past and present interests of mine. 
Many items are repeated across sections.</p>

    </li>
  
    <li>
      <h2><a href="/2025/02/22/randomized-numerical-algebra.html">Randomized Numerical Algebra</a></h2>
      <p>In this article we investigate randomized numerical algebra.</p>

    </li>
  
    <li>
      <h2><a href="/2025/01/04/universal-approximation-theorems.html">Universal Approximation Theorems</a></h2>
      <p>In this article we investigate universal approximation theorems.</p>

    </li>
  
    <li>
      <h2><a href="/2024/10/13/online-learning.html">Online Learning</a></h2>
      <p>In this article we investigate online learning.</p>

    </li>
  
    <li>
      <h2><a href="/2024/10/12/transfer-learning.html">Transfer Learning</a></h2>
      <p>In this article we investigate transfer learning.</p>

    </li>
  
    <li>
      <h2><a href="/2024/10/12/KRR.html">Kernel Ridge Regression</a></h2>
      <p>In this article we investigate kernel ridge regression.</p>

    </li>
  
    <li>
      <h2><a href="/2024/08/14/causal-inference.html">Causal Inference</a></h2>
      <p>In this article we investigate causal inference.</p>

    </li>
  
    <li>
      <h2><a href="/2024/08/14/Malliavin-calculus.html">Malliavin Calculus</a></h2>
      <p>In this article we investigate Malliavin calculus, which is a set of mathematical techniques and ideas that extend the calculus of variations from deterministic functions to stochastic processes.</p>

    </li>
  
    <li>
      <h2><a href="/2024/07/22/sampling.html">Sampling</a></h2>
      <p>In this article we investigate how to simulate probability concepts in computers.
We will also discuss its statistical applications.</p>

    </li>
  
    <li>
      <h2><a href="/2024/07/22/DMFT.html">Dynamical Mean Field Theory</a></h2>
      <p>In this article we investigate dynamical mean field theory.</p>

    </li>
  
    <li>
      <h2><a href="/2024/07/07/reinforcement-learning.html">Reinforcement Learning</a></h2>
      <p>Parallel to supervised learning, reinforcement learning is another modelling</p>

    </li>
  
    <li>
      <h2><a href="/2024/07/07/optimal-control.html">Optimal Control</a></h2>
      <p>Optimal Control</p>

    </li>
  
    <li>
      <h2><a href="/2024/07/05/splitting-schemes.html">Splitting Schemes</a></h2>
      <p>Consider the differential equation with phase state $M$</p>

    </li>
  
    <li>
      <h2><a href="/2024/07/05/exponential-integrators.html">Exponential Integrators</a></h2>
      

    </li>
  
    <li>
      <h2><a href="/2024/06/28/hypothesis-testing.html">Hypothesis Testing</a></h2>
      <p>What is the difference between Hypothesis Testing and Classification?</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/26/shapley-value.html">Shapley Value</a></h2>
      <p>Economical Ideas.</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/26/random-matrix.html">Random Matrix</a></h2>
      <p>Let us first recall some classical result on random matrix.</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/26/markov-geometry.html">Geometry of Markov Chains</a></h2>
      <p>This is my note for the book <a href="https://link.springer.com/book/10.1007/978-3-319-00227-9">Analysis and Geometry of Markov Diffusion Operators</a>.
This note mean to demonstrate the basic ideas and calculations, simplifying the technical details to as few lines as possible while maintaining rigor.</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/17/dyson-series.html">Dyson Series</a></h2>
      <p>Some quantum mechanical calculation.</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/13/watermark.html">Watermarks for LLM</a></h2>
      <p>Statistical watermark leverage the psuedorandomness during decoding.</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/05/convex-optimization.html">Convex Optimization</a></h2>
      <p>Consider the general constrained optimization problem (P) shown below, where we have not assumed anything regarding
the functions $f$, $h$, $l$ (like convexity).</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/03/stability-of-Lindblad-equation.html">Stability of Lindblad's Equation</a></h2>
      <p>Under certain conditions the evolution of a quantum system interacting with its environment can be described by a quantum dynamical semigroup and shown to satisfy a Lindblad master equation</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/02/boolean-analysis.html">Boolean Analysis</a></h2>
      <p>Denote</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/01/traditional-machine-learing-theory.html">Traditional Machine Learning Theory</a></h2>
      <p>metric entropy, chaining…</p>

    </li>
  
    <li>
      <h2><a href="/2024/06/01/continuous-limit-regret-analysis.html">Continuous Regret Analysis</a></h2>
      <p>In <strong>online learning</strong>, the data is provided in a sequential order, and the goal of the learner is to make online decisions to minimize overall <em>regrets</em>.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/24/Lyapunov-function.html">Lyapunov Functions</a></h2>
      <p>How to derive an estimate better than Gronwall’s inequality?</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/24/Hypocoercivity.html">Hypocoercivity</a></h2>
      <p>Hypocoercivity is a mathematical concept used to describe the behavior of certain dynamical systems and partial differential equations (PDEs) that do not exhibit traditional coercivity properties but still exhibit convergence to equilibrium over time. 
In the study of kinetic equations and systems where traditional coercive methods (like those used in parabolic PDEs) do not apply, we may resort to hypocoercivity.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/17/Laplace-asymptotic.html">Deriving the Asymptotics</a></h2>
      <p>Deriving the asymptotics</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/11/tail-estimation.html">Estimation of Heavy Tails</a></h2>
      <p>In this article, I breifly introduce some current methods of estimating the tail of a heavy tail distributions.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/11/spectral-representation-stationary-process.html">The Spectral Representation of a Stationary Prcoess</a></h2>
      <p>The spectral representation of a stationary process ${X_t}$ essentially decomposes ${X_t}$ into a sum of sinusoidal components with uncorrelated random coefficients. This is an analogue of the more familiar Fourier representation of deterministic functions.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/11/le-cam-method.html">Le Cam Methods</a></h2>
      <p>In this article, I breifly explain the idea of Le Cam methods.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/11/double-machine-learning.html">Double Machine Learning</a></h2>
      <p>In this article, I breifly explain the idea of double machine learning.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/10/koopman-operator.html">Koopman Operator</a></h2>
      <p>In this article, I breifly explain the theory of Koopman Operator.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/05/estimators.html">Point Estimation</a></h2>
      <p>In this article, I breifly review the theory of point estimation.</p>

    </li>
  
    <li>
      <h2><a href="/2024/05/04/classical-gradient-descent.html">Classical Gradient Descent Analysis</a></h2>
      <p>In this article, I recap the classical gradient descent analysis.</p>

    </li>
  
    <li>
      <h2><a href="/2024/04/13/NTK.html">NTK</a></h2>
      <p>In this article, we explain the Neural Tangent Kernel.</p>

    </li>
  
    <li>
      <h2><a href="/2024/04/11/PAC-Bayesian.html">PAC Bayesian</a></h2>
      <p>In this article, we explain the PAC-Bayesian generalization bound.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/30/quantum-computing.html">Quantum Computing</a></h2>
      <p>In this note, we first recall the basic postulates of quantum computing.
Then we mainly focus on the mathematical structures behind it.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/29/feature-learning.html">Feature Learning</a></h2>
      <p>One notable difference between traditional machine learning algorithms and deep learning is that in deep learning, the features are learnable and can be interpreted.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/28/symmetry-in-nn.html">Symmetry in Deep Learning Theory</a></h2>
      <p>We can leverage symmetry.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/18/completely-positive-map.html">Completely Positive Map</a></h2>
      <p>In this article, I describe $C^*$-algebra, complete positive map, and its relationship to quantum mechanics.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/12/Circuit-Complexity.html">Circuit Complexity</a></h2>
      <p>Circuit complexity is a measure of how hard it is to express or compute a boolean function.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/11/Communication-Complexity.html">Communication Complexity</a></h2>
      <p>Communication complexity was first introduced by Andrew Yao in 1973,
while studying the problem of computation distributed among several machines.</p>

    </li>
  
    <li>
      <h2><a href="/2024/03/07/diffusion-models.html">Diffusion Models</a></h2>
      <p>Let us first recall how to reverse a classical diffusion process.</p>

    </li>
  
    <li>
      <h2><a href="/2023/09/04/great-researchers.html">Great Researchers</a></h2>
      <p>In this series I try to imagine and examine the great figures in the history as researchers. The goal is not to diefy anyone, but to examine great figures in the history and to inspire researchers today.</p>

    </li>
  
    <li>
      <h2><a href="/2023/09/04/characteristics-of-research.html">Unlocking the Essential Characteristics of Doing Research</a></h2>
      <p>This is how I understand research.</p>

    </li>
  
    <li>
      <h2><a href="/2018/02/10/investigation-geometry.html">Investigation in Plane Geometry</a></h2>
      <p>This article records some of my proofs in plane geometry.</p>

    </li>
  
    <li>
      <h2><a href="/2017/10/04/new-geometry-problems.html">New Geometry Problems</a></h2>
      <p>This article records some problems in plane geometry that I, my friend Yonggang Ren, and my middle school teacher Lingfeng Chen have discovered.</p>

    </li>
  
    <li>
      <h2><a href="/2017/09/30/IMO-revisited.html">An IMO Problem Revisited</a></h2>
      <p>This article records some solutions of a problem in plane geometry that I discovered.</p>

    </li>
  
</ul>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="/assets/js/mathjax-config.js" defer></script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script> 


        </td>
      </tr>
    </table>
    <footer> <!-- start custom footer snippets --> 
      <a href="https://mapmyvisitors.com/web/1bvv0"  title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=ty1HEYbr6YNyZjlqemC1gLlTmBlR4TyXRKuxWL40zj8&cl=ffffff" /></a>
    </footer>
  </body>
</html>