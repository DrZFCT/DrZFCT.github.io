<!DOCTYPE html>
<html>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/assets/js/mathjax-config.js" defer></script>
  <script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script> 

     
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Point Estimation</title>
        <meta name="viewport" content="width=device-width">

        
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/assets/css/post.css">
        
    </head>

    <body>
        <header class="site-header">

            <div class="wrap">
                
              <div style="float:left; margin-top:10px; margin-right:10px;">
                
              </div>

              <div class="site-nav"><nav>
                  <div class="menu-item">
                    <a href="/blog.html" >Blog
                    </a>
                  </div>

              </nav></div>
          
              
            </div>
          
          </header>

    <div class="page-content">
      <div class="wrap">
        <div class="post">

            <header class="post-header">
              <h1>Point Estimation</h1>
              <p class="meta">May 5, 2024 • Kaizhao Liu</p>
            </header>
          
            <article class="post-content">
            <p>Let $X = (X_1, \cdots , X_n)$ be a sample from a population
$\PP_\theta \in \cP$, where $\theta \in \Theta$.
An estimator of $\gamma(\theta)$ is a measurable function from the range of $X$ to $\gamma(\Theta)$.</p>

<p>We use squared error loss (MSE)</p>

\[R_T(\theta)=\EE [(T-\gamma(\theta))^2|\theta]\]

<p>throughout the article.</p>

<h1 id="minimaxity">Minimaxity</h1>

<p>For a parametric family $\PP_\theta \in \cP$ and an estimator $T\in \cT$ among a class of estimators $T\in \cT$, the maximum risk of $T$ is defined to be the worst case scenario:</p>

\[\sup_{\theta\in \Theta} R_T(\theta) .\]

<p>The minimax risk is defined to be</p>

\[R_n=\inf_{T\in \cT} \sup_{\theta\in \Theta} R_T(\theta) .\]

<p>An esimator $T$ is minimax optimal if its maximum risk achieves the minimax rate.</p>

<p>In general, it is hard to find a minimax optimal estimator. Therefore, we are appeal to some weaker theoretical properties instead.</p>

<p>For example, when finding an exact minimax estimator is difficult, we may also find a minimax estimator <em>asymptotically</em>, i.e $\hat T$ such that</p>

\[\sup_{\theta\in \Theta} R_{\hat T}(\theta)\asymp \inf_{T\in \cT} \sup_{\theta\in \Theta} R_T(\theta).\]

<p>Some other weaker theoretical properties is explained in the next section.</p>

<h1 id="admissibility-unbiasedness-consistency">Admissibility, Unbiasedness, Consistency</h1>

<p>A decision rule $T \in \cT$ is called <strong>admissible</strong> if and only if there does not exist any $T’ \in \cT$ that has samller risk than $T$ uniformly over $\PP_\theta$.</p>

<p>Here is the celebrated theorem of <em>Rao and Blackwell</em>, which states that any admissible estimator must be functions of sufficient statistics: 
<em>Let $T$ be a sufficient statistic for $\PP_\theta \in \cP$, $T_0$ be
an estimator satisfying \(\EE_{\PP_\theta} \|T_0\| &lt; \infty\) for all $\PP \in \cP$. Let $T_1 = \EE [T_0(X) | T ]$ . Then
$R_{T_1}(\theta) \leq  R_{T_0} (\theta )$.</em></p>

<p>An estimator is said to be <strong>unbiased</strong> if and only if</p>

\[\EE_{\PP_\theta} T(X)=\gamma (\theta).\]

<p>The UMVUE is defined to be a minimax optimal estimator among the class of all unbiased estimators. Statisticians have developed some interesting methods to find UMVUEs, see <em>Lehmann–Scheffé theorem</em> for example.</p>

<p>The minimum risk of an unbiased estimator is constrained by <em>Cramér–Rao lower bound</em>.</p>

<p>If $T(X)\to_P \gamma(\theta)$ as $n\to\infty$, then $T(X)$ is called <strong>consistent</strong>. When dealing with large sample properties like this, a statistic $T(X)$ is often denoted by $T_n$ to emphasize its dependence on the sample size $n$.</p>

<h1 id="moment-estimation-mle">Moment Estimation, MLE</h1>

<p>The above criteria provide guidelines of finding good estimators, but they do not provide us any specific procedure to construct an estimator.</p>

<p>In practice, the most intuitive ways of constructing estimators are moment estimation and maximum likelihood estimation (MLE).</p>

<p>Moment estimation works by comparing population moments and sample moments.</p>

<p>MLE works by maximizing the likelihood.</p>

<h1 id="bayesian-statistics">Bayesian Statistics</h1>

<p>Take the prior distribution to be continuous with density $\pi$, an arbitrary probability density on $\Theta$.
The Bayesian risk of an estimator $T$ for $\gamma(θ)$ is defined as the weighted average of the
risk $R_T(\theta)$ with the weight $\pi$.</p>

<p>The <strong>Bayesian estimator</strong> w.r.t. the prior density $\pi$ is the estimator $T_{\text{Bayes}}$
that minimizes $R_{\text{Bayes}}(\pi, T )$ over all estimators $T$. A good property of Bayesian estiamtors is that they can be given explicitly by the posterior mean:</p>

\[T_{\text{Bayes}}(X)=\EE[\gamma(\theta)|X].\]

<p>An interesting connection between bayesian estimator and minimax estimator is the following.
Suppose that $T$ is the Bayes estimator for $\gamma(\theta)$ w.r.t some prior. If the risk $R_T$ does not depend on $\theta$, then $T$ is minimax optimal (among all estimators).</p>

            </article>
          


      </div>
    </div>

    <footer class="site-footer">

        <div class="wrap">

      
          <div class="footer-col-2 column">
            
          </div>
      
      
        </div>
      
      </footer>

    </body>
</html>
