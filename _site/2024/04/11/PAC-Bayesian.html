<!DOCTYPE html>
<html>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/assets/js/mathjax-config.js" defer></script>
  <script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script> 

     
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>PAC Bayesian</title>
        <meta name="viewport" content="width=device-width">

        
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/assets/css/post.css">
        
    </head>

    <body>
        <header class="site-header">

            <div class="wrap">
                
              <div style="float:left; margin-top:10px; margin-right:10px;">
                
              </div>

              <div class="site-nav"><nav>
                  <div class="menu-item">
                    <a href="/blog.html" >Blog
                    </a>
                  </div>

              </nav></div>
          
              
            </div>
          
          </header>

    <div class="page-content">
      <div class="wrap">
        <div class="post">

            <header class="post-header">
              <h1>PAC Bayesian</h1>
              <p class="meta">Apr 11, 2024 • Kaizhao Liu</p>
            </header>
          
            <article class="post-content">
            <p>In this article, we explain the PAC-Bayesian generalization bound.</p>

<h1 id="pac-bayesian-generalization-bound">PAC-Bayesian Generalization Bound</h1>

<p>Let $l:\cH\times \cZ \to [0,1]$ be a loss function and $\PP$ be a prior distribution over $\cH$.
Then, for any $\delta\in (0,1)$, with probability at least $1-\delta$ over the sampling of dataset $S$,
we have for any $\QQ \in \cP(\cH)$, it holds that</p>

\[L(\QQ)\leq \hat{L}(\QQ)+\sqrt{\frac{\text{KL}(\QQ||\PP)+\log(1/\delta)}{2n}}.\]

<p>The key point is that $\QQ$ can depend on the dataset $S$ but the prior $\PP$ can not. So one can optimize $\PP$ to minimize the KL divergence.</p>

<p>Why is $L(\QQ)=\EE_{h\sim\QQ} L(h)$ relavent? For example, model trained by SGD are influenced by both random initialization and sampling randomness, so the output model can be regarded as a distribution $\QQ$.</p>

<h1 id="donsker-and-varadhans-variational-principle">Donsker and Varadhan’s Variational Principle</h1>

<p>The key point in the proof of PAC-Bayesian Generalization bound is the following inequality, which can be regarded as a generalization of Jensen’s inequality:</p>

\[e^{\EE_p [V]}\leq \EE_\pi [e^V]e^{\text{KL}(p||\pi)}\]

<p>For any pair of distribution $p,\pi\in \cP(\cX)$.</p>

<p>This result follows from the variational representation of Gibbs distribution.
Let $V:\cX\to\RR$ be an (negative) energy function and $\pi\in\cP(\cX)$ be an arbitary underlying distribution. The corresponding Gibbs distribution is given by</p>

\[\frac{\mathrm{d} \pi_V}{\mathrm{d} \pi}=\frac{e^{V(x)}}{\EE_\pi[e^V]}.\]

<p>The Gibbs distribution can be characterized by the following variational principle:</p>

\[\pi_V=\arg\max_{p\in \cP(\cX)}(\EE_p[V]-\text{KL}(p||\pi)),\]

<p>with the optimal value</p>

\[\log \EE_\pi [e^V] = \sup_{p\in \cP(\cX)} (\EE_p[V]-\text{KL}(p||\pi)).\]

<p>The proof of this variational principle is based on the following observation:</p>

\[\begin{align}
    \EE_p[V]-\text{KL}(p||\pi) &amp;= \int V(x)p(x)\mathrm dx - \int p(x)\log(\frac{p(x)}{\pi(x)})\mathrm dx \\
    &amp;= -\int p(x)\log(\frac{p(x)}{\pi(x)e^{V(x)}})\mathrm dx \\
    &amp;= -\text{KL}(p||\pi_V)+\log \EE_\pi [e^V] .
\end{align}\]

            </article>
          


      </div>
    </div>

    <footer class="site-footer">

        <div class="wrap">

      
          <div class="footer-col-2 column">
            
          </div>
      
      
        </div>
      
      </footer>

    </body>
</html>
