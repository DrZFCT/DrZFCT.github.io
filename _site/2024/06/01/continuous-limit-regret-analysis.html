<!DOCTYPE html>
<html>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/assets/js/mathjax-config.js" defer></script>
  <script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script> 

     
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Continuous Regret Analysis</title>
        <meta name="viewport" content="width=device-width">

        
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/assets/css/post.css">
        
    </head>

    <body>
        <header class="site-header">

            <div class="wrap">
                
              <div style="float:left; margin-top:10px; margin-right:10px;">
                
              </div>

              <div class="site-nav"><nav>
                  <div class="menu-item">
                    <a href="/blog.html" >Blog
                    </a>
                  </div>

              </nav></div>
          
              
            </div>
          
          </header>

    <div class="page-content">
      <div class="wrap">
        <div class="post">

            <header class="post-header">
              <h1>Continuous Regret Analysis</h1>
              <p class="meta">Jun 1, 2024 • Kaizhao Liu</p>
            </header>
          
            <article class="post-content">
            <p>In <strong>online learning</strong>, the data is provided in a sequential order, and the goal of the learner is to make online decisions to minimize overall <em>regrets</em>.</p>

<ul>
  <li>the learner is a function $A$ that returns the current prediction given the full history</li>
</ul>

<h1 id="online-linear-optimization">Online Linear Optimization</h1>

<p>At each round $t=1,\cdots,T$</p>
<ul>
  <li>The learner picks $x_t\in X$.</li>
  <li>The opponent picks reward $r_t\in [0,1]^d$.</li>
  <li>The learner observes $r_t$ and gets reward $x_t^\top r_t$.</li>
</ul>

<p>The regret is defined as</p>

\[R=\max_{x\in X} x^\top \sum_{t=0}^T r_t - \sum_{t=0}^T  x_t^Tr_t .\]

<p>Regret is defined as the difference between the reward that could have been achieved by a <strong>single</strong> action,
given the choices of the opponent, and what was actually achieved.</p>

<p>Note that the “best action” is chosen with full knowledge of the
opponent’s whole sequence of actions, whereas the action of the learner at time $t$ must be based solely on the past history $r_0,\cdots,r_{t-1}$.</p>

<p>Also note that the opponent can be adversarial as the reward $r_t$ can be based on the learner’s action $x_t$</p>

<p>Is there any game theoretical explanation???</p>

            </article>
          


      </div>
    </div>

    <footer class="site-footer">

        <div class="wrap">

      
          <div class="footer-col-2 column">
            
          </div>
      
      
        </div>
      
      </footer>

    </body>
</html>
